{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d07e8c",
   "metadata": {},
   "source": [
    "## Loading Data from data.zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444ed5dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5384/5384 [00:03<00:00, 1559.01it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6493/6493 [00:04<00:00, 1555.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from data import c3d\n",
    "train, validation, test = c3d.load_data()\n",
    "\n",
    "#load_data returns the data after splitting it into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462d18f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets, train_labels = train.features, train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca583335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680 training samples\n",
      "2227 validation samples\n",
      "2970 testing samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"{train.num_examples} training samples\")\n",
    "print(f\"{validation.num_examples} validation samples\")\n",
    "print(f\"{test.num_examples} testing samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e4073",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab5eb45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import c3d\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "import re\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "import emoji\n",
    "from tqdm import trange \n",
    "import collections, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c881b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets, train_labels = train.features, train.labels\n",
    "val_tweets, val_labels = validation.features, validation.labels\n",
    "test_tweets, test_labels = test.features, test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a048e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 6680}), Counter({0: 2227}), Counter({0: 2970}))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(train_labels),collections.Counter(val_labels),collections.Counter(test_labels) # Data is balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d592f89",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e9d3eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@FahadHarthi1 Nothing like a coffee in the winter.\n",
      "['@', 'FahadHarthi1', 'Nothing', 'like', 'a', 'coffee', 'in', 'the', 'winter', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = train_tweets[0]\n",
    "print(sentence)\n",
    "words = word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c23e0a",
   "metadata": {},
   "source": [
    "### Converting to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90d1bc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@fahadharthi1 nothing like a coffee in the winter.\n"
     ]
    }
   ],
   "source": [
    "sentence = sentence.lower()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95219794",
   "metadata": {},
   "source": [
    "### Loading stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c990a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'himself', 'all', 'theirs', \"haven't\", 'from', 'a', 'shan', \"isn't\", 'such', 'ours', 'y', 'now', \"don't\", 'above', 'through', 'or', 're', \"you're\", 'ma', 'being', 'there', 's', 'having', 'hasn', \"needn't\", 'up', 'why', 'only', 'those', 'at', 'weren', 'can', 'themselves', 'were', 'shouldn', 've', \"couldn't\", 'your', 'ain', 'than', 'between', \"wouldn't\", 'i', \"shouldn't\", 'is', 'am', 'wouldn', 'not', 'll', \"shan't\", 'she', 'with', 'if', 'doing', 'aren', 'itself', 'what', 'him', 'until', 'mightn', \"doesn't\", \"weren't\", 'how', 'while', 'are', 'once', 'that', 'out', 'some', 'its', \"she's\", 'do', 'no', 'own', 'o', 'after', 'yourselves', 'below', 'during', 'the', 'won', 'but', 'he', 'by', \"mightn't\", 'ourselves', 'any', 'our', 'whom', 'has', 'couldn', 'didn', 'had', 'down', 'into', 'herself', 'was', 'under', 'hers', 'then', 'myself', 'her', 'in', 'more', 'these', 'they', 'so', 'on', 'will', 'did', \"it's\", 'hadn', 'needn', 'wasn', 'it', 'does', 'doesn', 'further', 'again', \"that'll\", \"didn't\", 'against', 'this', \"aren't\", 'haven', \"you'll\", 'we', 'before', 'my', 'over', \"should've\", 'few', 'of', 'to', 'because', 't', 'which', 'isn', 'same', 'me', 'just', 'yourself', 'yours', 'their', 'mustn', \"won't\", 'have', \"wasn't\", 'each', 'them', 'be', 'nor', 'when', 'very', 'off', 'both', 'd', \"you'd\", \"hasn't\", 'about', 'been', 'most', 'as', 'and', \"mustn't\", 'm', 'here', 'for', 'should', 'don', 'other', \"you've\", 'you', 'an', 'too', 'his', \"hadn't\", 'where', 'who'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ea642fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Tokenizer: ['@FahadHarthi1', 'Nothing', 'like', 'a', 'coffee', 'in', 'the', 'winter', '.']\n",
      "\n",
      "Social tokenizer: ['@fahadharthi1', 'nothing', 'like', 'a', 'coffee', 'in', 'the', 'winter', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nithy\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages\\ekphrasis\\classes\\tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    }
   ],
   "source": [
    "sentence = train_tweets[0]\n",
    "tknzr = TweetTokenizer()\n",
    "social_tokenizer = SocialTokenizer(lowercase=True).tokenize\n",
    "\n",
    "print(\"Tweet Tokenizer:\",tknzr.tokenize(sentence))\n",
    "print()\n",
    "print(\"Social tokenizer:\",social_tokenizer(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c5a7e",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "877d47e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@fahadharthi1\n",
      "noth\n",
      "like\n",
      "a\n",
      "coffe\n",
      "in\n",
      "the\n",
      "winter.\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "for word in sentence.split():\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8c08c",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ad9c250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@FahadHarthi1\n",
      "Nothing\n",
      "like\n",
      "a\n",
      "coffee\n",
      "in\n",
      "the\n",
      "winter.\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in sentence.split():\n",
    "    print(lemmatizer.lemmatize(word, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d82cf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Dani_TwiHard I told everyone, I have been diagnosed with \"Atypical Depression\". You can google it, that explains most dumb stuff I say =('"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", train_tweets[9]).split())\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b2435",
   "metadata": {},
   "source": [
    "### Loading spell checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b22ade6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word statistics files not found!\n",
      "Downloading... done!\n",
      "Unpacking... done!\n",
      "Reading english - 1grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams C:\\Users\\nithy\\.ekphrasis\\stats\\english\\counts_1grams.txt\n"
     ]
    }
   ],
   "source": [
    "sp = SpellCorrector(corpus=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036eb68",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfd70ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_delete = ['theirs', 'she', 'of', \n",
    "                  'all', 'her', 'ourselves', 'that', 'some', 'your', \n",
    "                  'what', 'or', 'me',  'now', 'after',\n",
    "                  'until', 'them', 'through', 'who', 'herself', 'he', \n",
    "                   'y', 'each', 'under', 'hers', 'other', 'down', \n",
    "                  'this', 'their', 'as', 'on','few', 'which', 'further', \n",
    "                  'whom', 'its', 'so', 'yourselves', 'because', 'it', 'both', 'in', 'nor', \n",
    "                    'yours', 'yourself', 'before','since', \n",
    "                  'there', 'himself', 'then', \n",
    "                  'him', 'over',  'here',  'an',  'into','next','d','u','r','im','m','have', \n",
    "                  'the', 'again','such', 'myself', 'they', \n",
    "                  'we', 'those', 'between', 'once','even','have'\n",
    "                   'how', 'from',  'ours', 'during','be','ama','r','i','do','but',\n",
    "                  'his', 'against', 'below',  'to', 'about', \n",
    "                   'by', 'i', 'where', 'a', 'very', 'our', 'my', 'for', 'and','ur'\n",
    "                  'while', 'only', 'up', 'these', 'just', 'same','how',\n",
    "                  'you', 'themselves', 'above', 'with',  'than', \n",
    "                  'own', 'out', 'when', 'any', 'too', 'o', 'at']\n",
    "def load_dict_contractions():\n",
    "    \n",
    "    return {\n",
    "        \"ain't\":\"is not\",\"amn't\":\"am not\",\"aren't\":\"are not\",\"can't\":\"cannot\",\"'cause\":\"because\",\"couldn't\":\"could not\",\n",
    "        \"couldn't've\":\"could not have\",\"could've\":\"could have\",\"daren't\":\"dare not\",\"daresn't\":\"dare not\",\"dasn't\":\"dare not\",\"didn't\":\"did not\",\"doesn't\":\"does not\",\n",
    "        \"don't\":\"do not\",\"e'er\":\"ever\",\"em\":\"them\",\"everyone's\":\"everyone is\",\"finna\":\"fixing to\",\n",
    "        \"gimme\":\"give me\",\"gonna\":\"going to\", \"gon't\":\"go not\",\"gotta\":\"got to\",  \"hadn't\":\"had not\", \"hasn't\":\"has not\",\"haven't\":\"have not\",\n",
    "        \"he'd\":\"he would\", \"he'll\":\"he will\",\"he's\":\"he is\", \"he've\":\"he have\",\"how'd\":\"how would\",\n",
    "        \"how'll\":\"how will\", \"how're\":\"how are\",\"how's\":\"how is\", \"I'd\":\"I would\", \"I'll\":\"I will\", \"I'm\":\"I am\",\n",
    "        \"I'm'a\":\"I am about to\",\"I'm'o\":\"I am going to\",\"isn't\":\"is not\",\"it'd\":\"it would\",\n",
    "        \"it'll\":\"it will\", \"it's\":\"it is\",\"I've\":\"I have\",\"kinda\":\"kind of\",\"let's\":\"let us\",\"mayn't\":\"may not\",\n",
    "        \"may've\":\"may have\",\"mightn't\":\"might not\",\"might've\":\"might have\", \"mustn't\":\"must not\",\"mustn't've\":\"must not have\",\n",
    "        \"must've\":\"must have\",\"needn't\":\"need not\", \"ne'er\":\"never\", \"o'\":\"of\", \"o'er\":\"over\",\"ol'\":\"old\",\n",
    "        \"oughtn't\":\"ought not\",\"shalln't\":\"shall not\",\"shan't\":\"shall not\",\"she'd\":\"she would\",\"she'll\":\"she will\",\"she's\":\"she is\",\n",
    "        \"shouldn't\":\"should not\",\"shouldn't've\":\"should not have\",\"should've\":\"should have\",\"somebody's\":\"somebody is\",\n",
    "        \"someone's\":\"someone is\",\"something's\":\"something is\",\"that'd\":\"that would\",\"that'll\":\"that will\",\n",
    "        \"that're\":\"that are\",\"that's\":\"that is\",\"there'd\":\"there would\",\"there'll\":\"there will\", \"there're\":\"there are\",\"there's\":\"there is\", \"these're\":\"these are\",\n",
    "        \"they'd\":\"they would\",\"they'll\":\"they will\",\"they're\":\"they are\", \"they've\":\"they have\",\"this's\":\"this is\",\n",
    "        \"those're\":\"those are\",\"'tis\":\"it is\",\"'twas\":\"it was\",\"wanna\":\"want to\",\"wasn't\":\"was not\",\"we'd\":\"we would\",\n",
    "        \"we'd've\":\"we would have\",\"we'll\":\"we will\",\"we're\":\"we are\",\"weren't\":\"were not\",\"we've\":\"we have\",\n",
    "        \"what'd\":\"what did\",\"what'll\":\"what will\",\"what're\":\"what are\",\"what's\":\"what is\",\n",
    "        \"what've\":\"what have\",\"when's\":\"when is\",\"where'd\":\"where did\",\"where're\":\"where are\",\"where's\":\"where is\",\n",
    "        \"where've\":\"where have\",\"which's\":\"which is\",\"who'd\":\"who would\",\"who'd've\":\"who would have\",\"who'll\":\"who will\",\n",
    "        \"who're\":\"who are\",\"who's\":\"who is\",\"who've\":\"who have\",\"why'd\":\"why did\",\"why're\":\"why are\",\"why's\":\"why is\",\n",
    "        \"won't\":\"will not\",\"wouldn't\":\"would not\",\"would've\":\"would have\",\"y'all\":\"you all\",\"you'd\":\"you would\",\n",
    "        \"you'll\":\"you will\",\"you're\":\"you are\",\"you've\":\"you have\",\"Whatcha\":\"What are you\",\"luv\":\"love\",\"sux\":\"sucks\"\n",
    "        ,\"shes\":\"she is\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c2e3e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nithy\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "               'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\",\n",
    "              'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    segmenter=\"twitter\",\n",
    "    # corpus from which the word statistics are going to be used\n",
    "    # for spell correction\n",
    "    corrector=\"english\",\n",
    "    unpack_hashtags=False,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True, \n",
    "    spell_correction=True,\n",
    "    tokenizer=SocialTokenizer(lowercase=False).tokenize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a633c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet,processor=text_processor):\n",
    "    tweet=emoji.demojize(tweet)\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    tweet = tweet.replace(\"â€™\",\"'\")\n",
    "    tweet = tweet.split()\n",
    "    contractions=load_dict_contractions()\n",
    "    tweet = [contractions[word] if word in contractions else word for word in tweet]\n",
    "    tweet = \" \".join(tweet) \n",
    "    tweet=\" \".join(text_processor.pre_process_doc(tweet))\n",
    "\n",
    "    # remove punctuations\n",
    "    tweet = re.sub(u'[{}]'.format('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^`{|}~'), u'',tweet)\n",
    "    # Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    s=''\n",
    "    for word in tweet.split():\n",
    "        s=s+\" \"+lemmatizer.lemmatize(word, pos='v')\n",
    "     # unuseful words removal\n",
    "    for w in words_to_delete:\n",
    "        pattern = r'\\b'+w+r'\\b'\n",
    "        s = re.sub(pattern, '', s)\n",
    "    #correct all multiple white spaces to a single white space\n",
    "    tweet = re.sub('[\\s]+', ' ', s)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "835b6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets (tweets):\n",
    "    clean_all_tweets=[]\n",
    "    for i in trange(len(tweets)):\n",
    "         clean_all_tweets.append(clean_tweet(tweets[i]))\n",
    "    return(clean_all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad0bb7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6680/6680 [00:06<00:00, 1101.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "@FahadHarthi1 Nothing like a coffee in the winter.\n",
      " user nothing like coffee winter\n",
      "\n",
      "\n",
      "I am Monty's Sally Cinnamon, isn't that right @DanMonty11 â˜ºï¸\n",
      " monty s sally cinnamon not right user smiling_face\n",
      "\n",
      "\n",
      "Thorn Breeder! I just bred a Thorn Dragon in DragonVale! Visit my park to check it out! https://t.co/i3dG1ohjOM\n",
      " thorn breeder breed thorn dragon dragonvale visit park check url\n",
      "\n",
      "\n",
      "I'm diagnosed with depression. I was like, the kush will take care of this sh*t\n",
      " diagnose depression like kush will take care sht censor\n",
      "\n",
      "\n",
      "Seguim a @camiamic un projecte comunitari d'educaciÃ³ en valors cÃ­vics i ambientals a l'Esquerra de l'Eixample i Sant Antoni\n",
      " seguim user un projecte comunitari educaciÃ³ en valors cÃ­vics ambientals l esquerra de l eixample sant antoni\n",
      "\n",
      "\n",
      "I feel so childish for being up this late like I don't have stuff to do TODAY ðŸ™„\n",
      " feel childish late like not stuff today face_with_rolling_eyes\n",
      "\n",
      "\n",
      "@BeamingBeautyx I have been diagnosed with Depression, Bipolar Disorder, and Borderline Personality Disorder\n",
      " user diagnose depression bipolar disorder borderline personality disorder\n",
      "\n",
      "\n",
      "Que ironÃ­a, ahora buscas amor en unos ojos que ni te miranðŸŽ¶\n",
      " que ironÃ­a ahora buscas amor en unos ojos que ni te miran musical_notes\n",
      "\n",
      "\n",
      "I was diagnosed with clinical depression and was prescribed to listen to Drake 24/7\n",
      " diagnose clinical depression prescribe listen drake number number\n",
      "\n",
      "\n",
      "@Dani_TwiHard  I told everyone, I have been diagnosed with \"Atypical Depression\". You can google it, that explains most dumb stuff I say =(\n",
      " user tell everyone diagnose atypical depression can google explain most dumb stuff say\n",
      "\n",
      "\n",
      "And there's this one guy who always drop by at our office. I usually ignore him because his personality piss me off. So he called my name\n",
      " one guy always drop office usually ignore personality piss off call name\n",
      "\n",
      "\n",
      "I was diagnosed with clinical depression almost a decade.\n",
      " diagnose clinical depression almost decade\n",
      "\n",
      "\n",
      "#DescribeYourselfin5words I'm a down asss bitchhh\n",
      " describeyourselfin5words elongate bitch elongate\n",
      "\n",
      "\n",
      "About 6 months ago, I was diagnosed with depression...\n",
      "I never fully understood what depressionâ€¦ https://t.co/cpbfPLuy12\n",
      " number months ago diagnose depression never fully understand depression â€¦ url\n",
      "\n",
      "\n",
      "so embarrassin puttin my makeup on in public cus i still put foundation on wit my hands n chuck eyeshadow on wit my fingers ðŸ¤—\n",
      " embarrassin puttin makeup public cus still put foundation wit hand n chuck eyeshadow wit finger smiling_face_with_open_hands\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Clean_train_tweets=clean_tweets(train_tweets)\n",
    "\n",
    "for i in range(15):\n",
    "    print()\n",
    "    print(train_tweets[i])\n",
    "    print(Clean_train_tweets[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e9ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
